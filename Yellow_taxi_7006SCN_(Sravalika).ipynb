{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "# 0) Mount Drive\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "import os, glob, shutil, time\n",
        "\n",
        "INPUT_CSV = \"/content/drive/MyDrive/2023_Yellow_Taxi_Trip_Data.csv\"\n",
        "PROJECT_DIR = \"/content/drive/MyDrive/7006SCN_YellowTaxi_Project_NEW1\"\n",
        "DATA_DIR = os.path.join(PROJECT_DIR, \"data\")\n",
        "OUT_DIR  = os.path.join(PROJECT_DIR, \"outputs\")\n",
        "\n",
        "os.makedirs(DATA_DIR, exist_ok=True)\n",
        "os.makedirs(OUT_DIR, exist_ok=True)\n",
        "\n",
        "print(\"INPUT_CSV:\", INPUT_CSV)\n",
        "print(\"PROJECT_DIR:\", PROJECT_DIR)\n",
        "print(\"DATA_DIR:\", DATA_DIR)\n",
        "print(\"OUT_DIR:\", OUT_DIR)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UY1YLaiMEAXh",
        "outputId": "2ee0285d-ef76-4b95-9043-65282610e7e6"
      },
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n",
            "INPUT_CSV: /content/drive/MyDrive/2023_Yellow_Taxi_Trip_Data.csv\n",
            "PROJECT_DIR: /content/drive/MyDrive/7006SCN_YellowTaxi_Project_NEW1\n",
            "DATA_DIR: /content/drive/MyDrive/7006SCN_YellowTaxi_Project_NEW1/data\n",
            "OUT_DIR: /content/drive/MyDrive/7006SCN_YellowTaxi_Project_NEW1/outputs\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# 1) Install Java + PySpark\n",
        "!apt-get -qq update\n",
        "!apt-get -qq install -y openjdk-11-jdk\n",
        "!pip -q install pyspark==3.5.1\n",
        "\n",
        "from pyspark.sql import SparkSession\n",
        "from pyspark.sql import functions as F\n",
        "\n",
        "spark = (\n",
        "    SparkSession.builder\n",
        "    .appName(\"7006SCN_YellowTaxi_Anomaly\")\n",
        "    .config(\"spark.driver.memory\", \"8g\")\n",
        "    .config(\"spark.executor.memory\", \"4g\")\n",
        "    .config(\"spark.sql.shuffle.partitions\", \"200\")\n",
        "    .config(\"spark.memory.offHeap.enabled\", \"true\")\n",
        "    .config(\"spark.memory.offHeap.size\", \"2g\")\n",
        "    .getOrCreate()\n",
        ")\n",
        "\n",
        "spark.sparkContext.setLogLevel(\"WARN\")\n",
        "print(\"Spark:\", spark.version)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fqmVQB2-EHIi",
        "outputId": "f62562a8-fd24-44a6-9080-a10d17722af8"
      },
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "W: Skipping acquire of configured file 'main/source/Sources' as repository 'https://r2u.stat.illinois.edu/ubuntu jammy InRelease' does not seem to provide it (sources.list entry misspelt?)\n",
            "Spark: 3.5.1\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# 2) Read CSV from Google Drive and LIMIT to 200k immediately (no full scan)\n",
        "df_raw = (spark.read\n",
        "          .option(\"header\", True)\n",
        "          .option(\"inferSchema\", True)\n",
        "          .csv(INPUT_CSV))\n",
        "\n",
        "TARGET_N = 200_000\n",
        "df = df_raw.limit(TARGET_N).cache()\n",
        "print(\"Rows loaded (should be 200k or less):\", df.count())\n",
        "df.show(5, truncate=False)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fWVw-yibEOVx",
        "outputId": "303f3027-95d2-43ed-9b88-5a0f4ddda647"
      },
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Rows loaded (should be 200k or less): 200000\n",
            "+--------+----------------------+----------------------+---------------+-------------+----------+------------------+------------+------------+------------+-----------+-----+-------+----------+------------+---------------------+------------+--------------------+-----------+\n",
            "|VendorID|tpep_pickup_datetime  |tpep_dropoff_datetime |passenger_count|trip_distance|RatecodeID|store_and_fwd_flag|PULocationID|DOLocationID|payment_type|fare_amount|extra|mta_tax|tip_amount|tolls_amount|improvement_surcharge|total_amount|congestion_surcharge|airport_fee|\n",
            "+--------+----------------------+----------------------+---------------+-------------+----------+------------------+------------+------------+------------+-----------+-----+-------+----------+------------+---------------------+------------+--------------------+-----------+\n",
            "|2       |01/01/2023 12:32:10 AM|01/01/2023 12:40:36 AM|1              |0.97         |1         |N                 |161         |141         |2           |9.3        |1.0  |0.5    |0.0       |0.0         |1.0                  |14.3        |2.5                 |0.0        |\n",
            "|2       |01/01/2023 12:55:08 AM|01/01/2023 01:01:27 AM|1              |1.1          |1         |N                 |43          |237         |1           |7.9        |1.0  |0.5    |4.0       |0.0         |1.0                  |16.9        |2.5                 |0.0        |\n",
            "|2       |01/01/2023 12:25:04 AM|01/01/2023 12:37:49 AM|1              |2.51         |1         |N                 |48          |238         |1           |14.9       |1.0  |0.5    |15.0      |0.0         |1.0                  |34.9        |2.5                 |0.0        |\n",
            "|1       |01/01/2023 12:03:48 AM|01/01/2023 12:13:25 AM|0              |1.9          |1         |N                 |138         |7           |1           |12.1       |7.25 |0.5    |0.0       |0.0         |1.0                  |20.85       |0.0                 |1.25       |\n",
            "|2       |01/01/2023 12:10:29 AM|01/01/2023 12:21:19 AM|1              |1.43         |1         |N                 |107         |79          |1           |11.4       |1.0  |0.5    |3.28      |0.0         |1.0                  |19.68       |2.5                 |0.0        |\n",
            "+--------+----------------------+----------------------+---------------+-------------+----------+------------------+------------+------------+------------+-----------+-----+-------+----------+------------+---------------------+------------+--------------------+-----------+\n",
            "only showing top 5 rows\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2b194b75",
        "outputId": "b7eb8c9a-c29b-4a0f-d130-a5ee420ccc55"
      },
      "source": [
        "# 3) Robust parsing + safe casting + feature engineering + cleaning\n",
        "import pyspark.sql.functions as F\n",
        "\n",
        "# --- 3.1 Robust datetime parsing (tries common formats) ---\n",
        "pickup_ts = F.coalesce(\n",
        "    F.to_timestamp(\"tpep_pickup_datetime\", \"MM/dd/yyyy hh:mm:ss a\"), # Added for AM/PM\n",
        "    F.to_timestamp(\"tpep_pickup_datetime\", \"yyyy-MM-dd HH:mm:ss\"),\n",
        "    F.to_timestamp(\"tpep_pickup_datetime\", \"MM/dd/yyyy HH:mm:ss\"),\n",
        "    F.to_timestamp(\"tpep_pickup_datetime\", \"MM/dd/yyyy HH:mm\"),\n",
        "    F.to_timestamp(\"tpep_pickup_datetime\", \"dd-MM-yyyy HH:mm\"),\n",
        "    F.to_timestamp(\"tpep_pickup_datetime\")  # last fallback\n",
        ")\n",
        "\n",
        "dropoff_ts = F.coalesce(\n",
        "    F.to_timestamp(\"tpep_dropoff_datetime\", \"MM/dd/yyyy hh:mm:ss a\"), # Added for AM/PM\n",
        "    F.to_timestamp(\"tpep_dropoff_datetime\", \"yyyy-MM-dd HH:mm:ss\"),\n",
        "    F.to_timestamp(\"tpep_dropoff_datetime\", \"MM/dd/yyyy HH:mm:ss\"),\n",
        "    F.to_timestamp(\"tpep_dropoff_datetime\", \"MM/dd/yyyy HH:mm\"),\n",
        "    F.to_timestamp(\"tpep_dropoff_datetime\", \"dd-MM-yyyy HH:mm\"),\n",
        "    F.to_timestamp(\"tpep_dropoff_datetime\")\n",
        ")\n",
        "\n",
        "df2 = (df\n",
        "       .withColumn(\"pickup_ts\", pickup_ts)\n",
        "       .withColumn(\"dropoff_ts\", dropoff_ts))\n",
        "\n",
        "print(\"pickup_ts nulls:\", df2.filter(F.col(\"pickup_ts\").isNull()).count())\n",
        "print(\"dropoff_ts nulls:\", df2.filter(F.col(\"dropoff_ts\").isNull()).count())\n",
        "\n",
        "# --- 3.2 Safe numeric casts (handles string columns too) ---\n",
        "num_cols = [\n",
        "    \"passenger_count\",\"trip_distance\",\"RatecodeID\",\"PULocationID\",\"DOLocationID\",\"payment_type\",\n",
        "    \"fare_amount\",\"extra\",\"mta_tax\",\"tip_amount\",\"tolls_amount\",\"improvement_surcharge\",\n",
        "    \"total_amount\",\"congestion_surcharge\",\"airport_fee\"\n",
        "]\n",
        "for c in num_cols:\n",
        "    if c in df2.columns:\n",
        "        df2 = df2.withColumn(c, F.col(c).cast(\"double\"))\n",
        "\n",
        "# --- 3.3 Features ---\n",
        "df2 = df2.withColumn(\n",
        "    \"trip_duration_min\",\n",
        "    (F.col(\"dropoff_ts\").cast(\"long\") - F.col(\"pickup_ts\").cast(\"long\")) / 60.0\n",
        ")\n",
        "\n",
        "df2 = df2.withColumn(\n",
        "    \"speed_mph\",\n",
        "    F.when(F.col(\"trip_duration_min\") > 0,\n",
        "           F.col(\"trip_distance\") / (F.col(\"trip_duration_min\") / 60.0)\n",
        "          ).otherwise(F.lit(None))\n",
        ")\n",
        "\n",
        "df2 = df2.withColumn(\n",
        "    \"tip_pct\",\n",
        "    F.when(F.col(\"fare_amount\") > 0, F.col(\"tip_amount\") / F.col(\"fare_amount\")).otherwise(F.lit(0.0))\n",
        ")\n",
        "\n",
        "# --- 3.4 Fill defaults ---\n",
        "df2 = df2.na.fill({\n",
        "    \"passenger_count\": 0.0,\n",
        "    \"RatecodeID\": 1.0,\n",
        "    \"store_and_fwd_flag\": \"N\",\n",
        "    \"congestion_surcharge\": 0.0,\n",
        "    \"airport_fee\": 0.0,\n",
        "    \"tip_amount\": 0.0,\n",
        "    \"tolls_amount\": 0.0,\n",
        "    \"extra\": 0.0,\n",
        "    \"mta_tax\": 0.0,\n",
        "    \"improvement_surcharge\": 0.0\n",
        "})\n",
        "\n",
        "# --- 3.5 Cleaning filters (realistic bounds) ---\n",
        "# NOTE: Keep bounds moderate to avoid dropping all rows.\n",
        "df3 = (df2\n",
        "    .filter(F.col(\"pickup_ts\").isNotNull() & F.col(\"dropoff_ts\").isNotNull())\n",
        "    .filter(F.col(\"trip_distance\").isNotNull())\n",
        "    .filter(F.col(\"total_amount\").isNotNull())\n",
        "    .filter((F.col(\"trip_distance\") > 0) & (F.col(\"trip_distance\") <= 60))\n",
        "    .filter((F.col(\"trip_duration_min\") > 0) & (F.col(\"trip_duration_min\") <= 360))\n",
        "    .filter((F.col(\"total_amount\") > 0) & (F.col(\"total_amount\") <= 800))\n",
        "    .cache()\n",
        ")\n",
        "\n",
        "print(\"Rows after cleaning:\", df3.count())\n",
        "df3.select(\"trip_distance\",\"trip_duration_min\",\"speed_mph\",\"total_amount\",\"tip_pct\").show(5, truncate=False)\n"
      ],
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "pickup_ts nulls: 0\n",
            "dropoff_ts nulls: 0\n",
            "Rows after cleaning: 194737\n",
            "+-------------+------------------+------------------+------------+------------------+\n",
            "|trip_distance|trip_duration_min |speed_mph         |total_amount|tip_pct           |\n",
            "+-------------+------------------+------------------+------------+------------------+\n",
            "|0.97         |8.433333333333334 |6.901185770750987 |14.3        |0.0               |\n",
            "|1.1          |6.316666666666666 |10.448548812664908|16.9        |0.5063291139240506|\n",
            "|2.51         |12.75             |11.811764705882352|34.9        |1.006711409395973 |\n",
            "|1.9          |9.616666666666667 |11.854419410745232|20.85       |0.0               |\n",
            "|1.43         |10.833333333333334|7.92              |19.68       |0.287719298245614 |\n",
            "+-------------+------------------+------------------+------------+------------------+\n",
            "only showing top 5 rows\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# 4) Create classification label: is_anomaly (0/1)\n",
        "#    We create a RULE-BASED label (common approach for anomaly detection training when no ground truth).\n",
        "#    You can explain this in the report as a proxy label for “suspicious trips”.\n",
        "\n",
        "# Anomaly rules (tunable):\n",
        "# - very high speed\n",
        "# - very long distance\n",
        "# - very high total amount\n",
        "# - very long duration\n",
        "df_labeled = (df3.withColumn(\n",
        "    \"is_anomaly\",\n",
        "    F.when(\n",
        "        (F.col(\"speed_mph\") >= 60) |\n",
        "        (F.col(\"trip_distance\") >= 30) |\n",
        "        (F.col(\"total_amount\") >= 250) |\n",
        "        (F.col(\"trip_duration_min\") >= 180),\n",
        "        F.lit(1.0)\n",
        "    ).otherwise(F.lit(0.0))\n",
        ")).cache()\n",
        "\n",
        "print(\"Label distribution:\")\n",
        "df_labeled.groupBy(\"is_anomaly\").count().show()\n",
        "\n",
        "# Keep ONLY 200k rows for the rest (still <=200k after cleaning)\n",
        "# If cleaning reduced rows, we keep what's available.\n",
        "df_final = df_labeled.cache()\n",
        "print(\"Final rows used:\", df_final.count())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5TJR0s0PFI0C",
        "outputId": "c83d171f-3d22-49c7-8563-36c1911b5b38"
      },
      "execution_count": 27,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Label distribution:\n",
            "+----------+------+\n",
            "|is_anomaly| count|\n",
            "+----------+------+\n",
            "|       0.0|194306|\n",
            "|       1.0|   431|\n",
            "+----------+------+\n",
            "\n",
            "Final rows used: 194737\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# 5) Save cleaned sample to Drive:\n",
        "#    - Parquet in DATA_DIR\n",
        "#    - Single CSV for Tableau in OUT_DIR\n",
        "\n",
        "PARQUET_PATH = os.path.join(DATA_DIR, \"yellow_taxi_200k_clean.parquet\")\n",
        "df_final.write.mode(\"overwrite\").parquet(PARQUET_PATH)\n",
        "print(\"Saved Parquet:\", PARQUET_PATH)\n",
        "\n",
        "CSV_TMP = os.path.join(OUT_DIR, \"sample_200k_for_tableau_csv_tmp\")\n",
        "(df_final\n",
        " .coalesce(1)\n",
        " .write.mode(\"overwrite\")\n",
        " .option(\"header\", True)\n",
        " .csv(CSV_TMP))\n",
        "\n",
        "part = glob.glob(CSV_TMP + \"/part-*.csv\")[0]\n",
        "CSV_OUT = os.path.join(OUT_DIR, \"sample_200k_for_tableau.csv\")\n",
        "shutil.copy(part, CSV_OUT)\n",
        "print(\"Saved Tableau CSV:\", CSV_OUT)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GyEYW2N0FSHB",
        "outputId": "37bbcf35-95cc-464a-f0a1-8e6d388e6075"
      },
      "execution_count": 28,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Saved Parquet: /content/drive/MyDrive/7006SCN_YellowTaxi_Project_NEW1/data/yellow_taxi_200k_clean.parquet\n",
            "Saved Tableau CSV: /content/drive/MyDrive/7006SCN_YellowTaxi_Project_NEW1/outputs/sample_200k_for_tableau.csv\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# 6) Build ML pipeline components (Spark MLlib)\n",
        "from pyspark.ml import Pipeline\n",
        "from pyspark.ml.feature import StringIndexer, OneHotEncoder, VectorAssembler, StandardScaler\n",
        "\n",
        "# Split\n",
        "train_df, test_df = df_final.randomSplit([0.8, 0.2], seed=42)\n",
        "train_df = train_df.cache()\n",
        "test_df  = test_df.cache()\n",
        "print(\"Train:\", train_df.count(), \"Test:\", test_df.count())\n",
        "\n",
        "# Columns\n",
        "categorical_cols = [\"VendorID\",\"RatecodeID\",\"store_and_fwd_flag\",\"payment_type\",\"PULocationID\",\"DOLocationID\"]\n",
        "numeric_cols = [\n",
        "    \"passenger_count\",\"trip_distance\",\"fare_amount\",\"extra\",\"mta_tax\",\"tip_amount\",\n",
        "    \"tolls_amount\",\"improvement_surcharge\",\"congestion_surcharge\",\"airport_fee\",\n",
        "    \"trip_duration_min\",\"speed_mph\",\"tip_pct\"\n",
        "]\n",
        "\n",
        "# Keep only existing columns (avoids errors if a column not present)\n",
        "categorical_cols = [c for c in categorical_cols if c in df_final.columns]\n",
        "numeric_cols = [c for c in numeric_cols if c in df_final.columns]\n",
        "\n",
        "indexers = [StringIndexer(inputCol=c, outputCol=f\"{c}_idx\", handleInvalid=\"keep\") for c in categorical_cols]\n",
        "encoder  = OneHotEncoder(\n",
        "    inputCols=[f\"{c}_idx\" for c in categorical_cols],\n",
        "    outputCols=[f\"{c}_ohe\" for c in categorical_cols],\n",
        "    handleInvalid=\"keep\"\n",
        ")\n",
        "\n",
        "assembler = VectorAssembler(\n",
        "    inputCols=numeric_cols + [f\"{c}_ohe\" for c in categorical_cols],\n",
        "    outputCol=\"features_raw\",\n",
        "    handleInvalid=\"keep\"\n",
        ")\n",
        "\n",
        "scaler = StandardScaler(inputCol=\"features_raw\", outputCol=\"features\", withStd=True, withMean=False)\n",
        "\n",
        "prep_stages = indexers + [encoder, assembler, scaler]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LtfGW4OQFWuy",
        "outputId": "ff110894-aa03-4d73-d7ba-6199e34c3368"
      },
      "execution_count": 29,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train: 155635 Test: 39102\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# 7) Evaluators + helper for metrics\n",
        "from pyspark.ml.evaluation import MulticlassClassificationEvaluator, BinaryClassificationEvaluator\n",
        "\n",
        "acc_eval = MulticlassClassificationEvaluator(labelCol=\"is_anomaly\", predictionCol=\"prediction\", metricName=\"accuracy\")\n",
        "f1_eval  = MulticlassClassificationEvaluator(labelCol=\"is_anomaly\", predictionCol=\"prediction\", metricName=\"f1\")\n",
        "auc_eval = BinaryClassificationEvaluator(labelCol=\"is_anomaly\", rawPredictionCol=\"rawPrediction\", metricName=\"areaUnderROC\")\n",
        "\n",
        "metrics = []\n",
        "ts = time.strftime(\"%Y-%m-%d %H:%M:%S\")"
      ],
      "metadata": {
        "id": "7QnydNn4GJwR"
      },
      "execution_count": 30,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 8) Model 1: Logistic Regression (separate training, no loop)\n",
        "from pyspark.ml.classification import LogisticRegression\n",
        "\n",
        "lr = LogisticRegression(labelCol=\"is_anomaly\", featuresCol=\"features\", maxIter=50, regParam=0.01, elasticNetParam=0.0)\n",
        "pipe_lr = Pipeline(stages=prep_stages + [lr])\n",
        "\n",
        "start = time.time()\n",
        "lr_model = pipe_lr.fit(train_df)\n",
        "lr_time = time.time() - start\n",
        "\n",
        "lr_pred = lr_model.transform(test_df)\n",
        "\n",
        "metrics.append({\n",
        "    \"timestamp\": ts,\n",
        "    \"task\": \"classification_anomaly\",\n",
        "    \"model\": \"LogisticRegression\",\n",
        "    \"accuracy\": float(acc_eval.evaluate(lr_pred)),\n",
        "    \"f1\": float(f1_eval.evaluate(lr_pred)),\n",
        "    \"auc\": float(auc_eval.evaluate(lr_pred)),\n",
        "    \"train_time_sec\": float(lr_time)\n",
        "})\n",
        "print(metrics[-1])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PUAMt0aqGMiZ",
        "outputId": "3eace3b7-f700-4938-dcfe-139739b913f6"
      },
      "execution_count": 31,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{'timestamp': '2026-02-27 07:55:41', 'task': 'classification_anomaly', 'model': 'LogisticRegression', 'accuracy': 0.998158661961025, 'f1': 0.9976308606603832, 'auc': 0.9779640548840709, 'train_time_sec': 37.22212100028992}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# 9) Model 2: Random Forest Classifier (separate training)\n",
        "from pyspark.ml.classification import RandomForestClassifier\n",
        "\n",
        "rf = RandomForestClassifier(labelCol=\"is_anomaly\", featuresCol=\"features\", numTrees=120, maxDepth=12, seed=42)\n",
        "pipe_rf = Pipeline(stages=prep_stages + [rf])\n",
        "\n",
        "start = time.time()\n",
        "rf_model = pipe_rf.fit(train_df)\n",
        "rf_time = time.time() - start\n",
        "\n",
        "rf_pred = rf_model.transform(test_df)\n",
        "\n",
        "metrics.append({\n",
        "    \"timestamp\": ts,\n",
        "    \"task\": \"classification_anomaly\",\n",
        "    \"model\": \"RandomForestClassifier\",\n",
        "    \"accuracy\": float(acc_eval.evaluate(rf_pred)),\n",
        "    \"f1\": float(f1_eval.evaluate(rf_pred)),\n",
        "    \"auc\": float(auc_eval.evaluate(rf_pred)),\n",
        "    \"train_time_sec\": float(rf_time)\n",
        "})\n",
        "print(metrics[-1])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "m-Gj0WYaGQJa",
        "outputId": "5924f630-50b7-4521-e1d3-d4d589a1631c"
      },
      "execution_count": 32,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{'timestamp': '2026-02-27 07:55:41', 'task': 'classification_anomaly', 'model': 'RandomForestClassifier', 'accuracy': 0.9978261981484323, 'f1': 0.9967404798616825, 'auc': 0.993736666822456, 'train_time_sec': 146.22498202323914}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# 10) Model 3: GBT Classifier (separate training)\n",
        "from pyspark.ml.classification import GBTClassifier\n",
        "\n",
        "gbt = GBTClassifier(labelCol=\"is_anomaly\", featuresCol=\"features\", maxIter=60, maxDepth=6, seed=42)\n",
        "pipe_gbt = Pipeline(stages=prep_stages + [gbt])\n",
        "\n",
        "start = time.time()\n",
        "gbt_model = pipe_gbt.fit(train_df)\n",
        "gbt_time = time.time() - start\n",
        "\n",
        "gbt_pred = gbt_model.transform(test_df)\n",
        "\n",
        "metrics.append({\n",
        "    \"timestamp\": ts,\n",
        "    \"task\": \"classification_anomaly\",\n",
        "    \"model\": \"GBTClassifier\",\n",
        "    \"accuracy\": float(acc_eval.evaluate(gbt_pred)),\n",
        "    \"f1\": float(f1_eval.evaluate(gbt_pred)),\n",
        "    \"auc\": float(auc_eval.evaluate(gbt_pred)),\n",
        "    \"train_time_sec\": float(gbt_time)\n",
        "})\n",
        "print(metrics[-1])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "T1dOuMTyGSgH",
        "outputId": "26daaeba-131e-4420-9a54-ff9e2eeb3cdb"
      },
      "execution_count": 33,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{'timestamp': '2026-02-27 07:55:41', 'task': 'classification_anomaly', 'model': 'GBTClassifier', 'accuracy': 0.9990281827016521, 'f1': 0.9989913529584007, 'auc': 0.9986523219893592, 'train_time_sec': 207.82260274887085}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# 11) Model 4: Linear SVC (separate training)\n",
        "# NOTE: LinearSVC does NOT output probability; it outputs rawPrediction.\n",
        "# AUC works (uses rawPrediction), accuracy & f1 work too.\n",
        "\n",
        "from pyspark.ml.classification import LinearSVC\n",
        "\n",
        "svc = LinearSVC(labelCol=\"is_anomaly\", featuresCol=\"features\", maxIter=80, regParam=0.05)\n",
        "pipe_svc = Pipeline(stages=prep_stages + [svc])\n",
        "\n",
        "start = time.time()\n",
        "svc_model = pipe_svc.fit(train_df)\n",
        "svc_time = time.time() - start\n",
        "\n",
        "svc_pred = svc_model.transform(test_df)\n",
        "\n",
        "metrics.append({\n",
        "    \"timestamp\": ts,\n",
        "    \"task\": \"classification_anomaly\",\n",
        "    \"model\": \"LinearSVC\",\n",
        "    \"accuracy\": float(acc_eval.evaluate(svc_pred)),\n",
        "    \"f1\": float(f1_eval.evaluate(svc_pred)),\n",
        "    \"auc\": float(auc_eval.evaluate(svc_pred)),\n",
        "    \"train_time_sec\": float(svc_time)\n",
        "})\n",
        "print(metrics[-1])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QNDyPX0zGUkg",
        "outputId": "05b0d1a7-ad3f-4d3c-a276-4ba0f355e6bd"
      },
      "execution_count": 34,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{'timestamp': '2026-02-27 07:55:41', 'task': 'classification_anomaly', 'model': 'LinearSVC', 'accuracy': 0.9979540688455834, 'f1': 0.9970458137745494, 'auc': 0.960583395774698, 'train_time_sec': 19.517208337783813}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# 12) Save metrics CSV to Drive\n",
        "import pandas as pd\n",
        "\n",
        "metrics_df = pd.DataFrame(metrics)\n",
        "METRICS_PATH = os.path.join(OUT_DIR, \"model_metrics_classification.csv\")\n",
        "metrics_df.to_csv(METRICS_PATH, index=False)\n",
        "\n",
        "print(\"Saved metrics:\", METRICS_PATH)\n",
        "metrics_df"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 192
        },
        "id": "hN9mot-0GVQH",
        "outputId": "08bca420-a5ec-4efa-f37d-302b133dc1db"
      },
      "execution_count": 35,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Saved metrics: /content/drive/MyDrive/7006SCN_YellowTaxi_Project_NEW1/outputs/model_metrics_classification.csv\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "             timestamp                    task                   model  \\\n",
              "0  2026-02-27 07:55:41  classification_anomaly      LogisticRegression   \n",
              "1  2026-02-27 07:55:41  classification_anomaly  RandomForestClassifier   \n",
              "2  2026-02-27 07:55:41  classification_anomaly           GBTClassifier   \n",
              "3  2026-02-27 07:55:41  classification_anomaly               LinearSVC   \n",
              "\n",
              "   accuracy        f1       auc  train_time_sec  \n",
              "0  0.998159  0.997631  0.977964       37.222121  \n",
              "1  0.997826  0.996740  0.993737      146.224982  \n",
              "2  0.999028  0.998991  0.998652      207.822603  \n",
              "3  0.997954  0.997046  0.960583       19.517208  "
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-3c49d309-f89a-4e5e-961c-72ea606ea76d\" class=\"colab-df-container\">\n",
              "    <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>timestamp</th>\n",
              "      <th>task</th>\n",
              "      <th>model</th>\n",
              "      <th>accuracy</th>\n",
              "      <th>f1</th>\n",
              "      <th>auc</th>\n",
              "      <th>train_time_sec</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>2026-02-27 07:55:41</td>\n",
              "      <td>classification_anomaly</td>\n",
              "      <td>LogisticRegression</td>\n",
              "      <td>0.998159</td>\n",
              "      <td>0.997631</td>\n",
              "      <td>0.977964</td>\n",
              "      <td>37.222121</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>2026-02-27 07:55:41</td>\n",
              "      <td>classification_anomaly</td>\n",
              "      <td>RandomForestClassifier</td>\n",
              "      <td>0.997826</td>\n",
              "      <td>0.996740</td>\n",
              "      <td>0.993737</td>\n",
              "      <td>146.224982</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>2026-02-27 07:55:41</td>\n",
              "      <td>classification_anomaly</td>\n",
              "      <td>GBTClassifier</td>\n",
              "      <td>0.999028</td>\n",
              "      <td>0.998991</td>\n",
              "      <td>0.998652</td>\n",
              "      <td>207.822603</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>2026-02-27 07:55:41</td>\n",
              "      <td>classification_anomaly</td>\n",
              "      <td>LinearSVC</td>\n",
              "      <td>0.997954</td>\n",
              "      <td>0.997046</td>\n",
              "      <td>0.960583</td>\n",
              "      <td>19.517208</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "    <div class=\"colab-df-buttons\">\n",
              "\n",
              "  <div class=\"colab-df-container\">\n",
              "    <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-3c49d309-f89a-4e5e-961c-72ea606ea76d')\"\n",
              "            title=\"Convert this dataframe to an interactive table.\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\" viewBox=\"0 -960 960 960\">\n",
              "    <path d=\"M120-120v-720h720v720H120Zm60-500h600v-160H180v160Zm220 220h160v-160H400v160Zm0 220h160v-160H400v160ZM180-400h160v-160H180v160Zm440 0h160v-160H620v160ZM180-180h160v-160H180v160Zm440 0h160v-160H620v160Z\"/>\n",
              "  </svg>\n",
              "    </button>\n",
              "\n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    .colab-df-buttons div {\n",
              "      margin-bottom: 4px;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "    <script>\n",
              "      const buttonEl =\n",
              "        document.querySelector('#df-3c49d309-f89a-4e5e-961c-72ea606ea76d button.colab-df-convert');\n",
              "      buttonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "      async function convertToInteractive(key) {\n",
              "        const element = document.querySelector('#df-3c49d309-f89a-4e5e-961c-72ea606ea76d');\n",
              "        const dataTable =\n",
              "          await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                    [key], {});\n",
              "        if (!dataTable) return;\n",
              "\n",
              "        const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "          '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "          + ' to learn more about interactive tables.';\n",
              "        element.innerHTML = '';\n",
              "        dataTable['output_type'] = 'display_data';\n",
              "        await google.colab.output.renderOutput(dataTable, element);\n",
              "        const docLink = document.createElement('div');\n",
              "        docLink.innerHTML = docLinkHtml;\n",
              "        element.appendChild(docLink);\n",
              "      }\n",
              "    </script>\n",
              "  </div>\n",
              "\n",
              "\n",
              "  <div id=\"id_3cfb58c4-aba0-477d-95da-2bc53e03050e\">\n",
              "    <style>\n",
              "      .colab-df-generate {\n",
              "        background-color: #E8F0FE;\n",
              "        border: none;\n",
              "        border-radius: 50%;\n",
              "        cursor: pointer;\n",
              "        display: none;\n",
              "        fill: #1967D2;\n",
              "        height: 32px;\n",
              "        padding: 0 0 0 0;\n",
              "        width: 32px;\n",
              "      }\n",
              "\n",
              "      .colab-df-generate:hover {\n",
              "        background-color: #E2EBFA;\n",
              "        box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "        fill: #174EA6;\n",
              "      }\n",
              "\n",
              "      [theme=dark] .colab-df-generate {\n",
              "        background-color: #3B4455;\n",
              "        fill: #D2E3FC;\n",
              "      }\n",
              "\n",
              "      [theme=dark] .colab-df-generate:hover {\n",
              "        background-color: #434B5C;\n",
              "        box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "        filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "        fill: #FFFFFF;\n",
              "      }\n",
              "    </style>\n",
              "    <button class=\"colab-df-generate\" onclick=\"generateWithVariable('metrics_df')\"\n",
              "            title=\"Generate code using this dataframe.\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M7,19H8.4L18.45,9,17,7.55,7,17.6ZM5,21V16.75L18.45,3.32a2,2,0,0,1,2.83,0l1.4,1.43a1.91,1.91,0,0,1,.58,1.4,1.91,1.91,0,0,1-.58,1.4L9.25,21ZM18.45,9,17,7.55Zm-12,3A5.31,5.31,0,0,0,4.9,8.1,5.31,5.31,0,0,0,1,6.5,5.31,5.31,0,0,0,4.9,4.9,5.31,5.31,0,0,0,6.5,1,5.31,5.31,0,0,0,8.1,4.9,5.31,5.31,0,0,0,12,6.5,5.46,5.46,0,0,0,6.5,12Z\"/>\n",
              "  </svg>\n",
              "    </button>\n",
              "    <script>\n",
              "      (() => {\n",
              "      const buttonEl =\n",
              "        document.querySelector('#id_3cfb58c4-aba0-477d-95da-2bc53e03050e button.colab-df-generate');\n",
              "      buttonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "      buttonEl.onclick = () => {\n",
              "        google.colab.notebook.generateWithVariable('metrics_df');\n",
              "      }\n",
              "      })();\n",
              "    </script>\n",
              "  </div>\n",
              "\n",
              "    </div>\n",
              "  </div>\n"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "dataframe",
              "variable_name": "metrics_df",
              "summary": "{\n  \"name\": \"metrics_df\",\n  \"rows\": 4,\n  \"fields\": [\n    {\n      \"column\": \"timestamp\",\n      \"properties\": {\n        \"dtype\": \"object\",\n        \"num_unique_values\": 1,\n        \"samples\": [\n          \"2026-02-27 07:55:41\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"task\",\n      \"properties\": {\n        \"dtype\": \"category\",\n        \"num_unique_values\": 1,\n        \"samples\": [\n          \"classification_anomaly\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"model\",\n      \"properties\": {\n        \"dtype\": \"string\",\n        \"num_unique_values\": 4,\n        \"samples\": [\n          \"RandomForestClassifier\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"accuracy\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 0.0005418560083267769,\n        \"min\": 0.9978261981484323,\n        \"max\": 0.9990281827016521,\n        \"num_unique_values\": 4,\n        \"samples\": [\n          0.9978261981484323\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"f1\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 0.0009971115923030354,\n        \"min\": 0.9967404798616825,\n        \"max\": 0.9989913529584007,\n        \"num_unique_values\": 4,\n        \"samples\": [\n          0.9967404798616825\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"auc\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 0.017203229309924625,\n        \"min\": 0.960583395774698,\n        \"max\": 0.9986523219893592,\n        \"num_unique_values\": 4,\n        \"samples\": [\n          0.993736666822456\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"train_time_sec\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 89.72534961951094,\n        \"min\": 19.517208337783813,\n        \"max\": 207.82260274887085,\n        \"num_unique_values\": 4,\n        \"samples\": [\n          146.22498202323914\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    }\n  ]\n}"
            }
          },
          "metadata": {},
          "execution_count": 35
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# 13) Save predictions for Tableau (use BEST model by AUC)\n",
        "# Pick best model name from metrics_df\n",
        "best_model_name = metrics_df.sort_values(\"auc\", ascending=False).iloc[0][\"model\"]\n",
        "print(\"Best model by AUC:\", best_model_name)\n",
        "\n",
        "# Choose fitted model object\n",
        "best_fitted = None\n",
        "if best_model_name == \"LogisticRegression\":\n",
        "    best_fitted = lr_model\n",
        "elif best_model_name == \"RandomForestClassifier\":\n",
        "    best_fitted = rf_model\n",
        "elif best_model_name == \"GBTClassifier\":\n",
        "    best_fitted = gbt_model\n",
        "else:\n",
        "    best_fitted = svc_model\n",
        "\n",
        "pred_out = (best_fitted.transform(test_df)\n",
        "    .select(\n",
        "        \"VendorID\",\"pickup_ts\",\"dropoff_ts\",\n",
        "        \"PULocationID\",\"DOLocationID\",\n",
        "        \"passenger_count\",\"trip_distance\",\"trip_duration_min\",\"speed_mph\",\n",
        "        \"fare_amount\",\"tip_amount\",\"total_amount\",\"tip_pct\",\n",
        "        \"is_anomaly\",\n",
        "        F.col(\"prediction\").alias(\"pred_is_anomaly\")\n",
        "    )\n",
        ")\n",
        "\n",
        "# Save single CSV for Tableau\n",
        "PRED_CSV_TMP = os.path.join(OUT_DIR, \"test_predictions_csv_tmp\")\n",
        "(pred_out.coalesce(1)\n",
        " .write.mode(\"overwrite\")\n",
        " .option(\"header\", True)\n",
        " .csv(PRED_CSV_TMP))\n",
        "\n",
        "part = glob.glob(PRED_CSV_TMP + \"/part-*.csv\")[0]\n",
        "PRED_CSV = os.path.join(OUT_DIR, \"test_predictions_for_tableau.csv\")\n",
        "shutil.copy(part, PRED_CSV)\n",
        "\n",
        "print(\"Saved prediction CSV:\", PRED_CSV)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "H4ET6BRtGbox",
        "outputId": "08df31e4-75ab-4a08-832a-23c0bbaa9503"
      },
      "execution_count": 36,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Best model by AUC: GBTClassifier\n",
            "Saved prediction CSV: /content/drive/MyDrive/7006SCN_YellowTaxi_Project_NEW1/outputs/test_predictions_for_tableau.csv\n"
          ]
        }
      ]
    }
  ]
}